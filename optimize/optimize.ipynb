{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKm9osY3nKSC",
        "outputId": "cd552dfa-3c43-46f1-e936-e2e2af21b645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 31 18:04:11 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 torchtext==0.14.1\n",
        "!pip install torchdata==0.5.1\n",
        "!python -m pip install \"optimum[openvino,nncf]\"\n",
        "!pip install diffusers\n",
        "!pip install accelerate\n",
        "!pip install tomesd\n",
        "!pip install nncf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjfu6hyQUZM9",
        "outputId": "665b67c6-c1c1-413b-ebd1-abb315fc9cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m943.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.14.1\n",
            "  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.41.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (3.4)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision, torchtext, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchaudio-0.13.1 torchtext-0.14.1 torchvision-0.14.1\n",
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (0.41.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Installing collected packages: portalocker, torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1\n",
            "Collecting optimum[nncf,openvino]\n",
            "  Downloading optimum-1.10.1.tar.gz (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from optimum[nncf,openvino])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[nncf,openvino]) (1.11.1)\n",
            "Collecting transformers[sentencepiece]>=4.26.0 (from optimum[nncf,openvino])\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum[nncf,openvino]) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum[nncf,openvino]) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum[nncf,openvino]) (1.22.4)\n",
            "Collecting huggingface-hub>=0.8.0 (from optimum[nncf,openvino])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from optimum[nncf,openvino])\n",
            "  Downloading datasets-2.14.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum-intel[openvino]>=1.10.1 (from optimum[nncf,openvino])\n",
            "  Downloading optimum-intel-1.10.1.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[nncf,openvino]) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[nncf,openvino]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[nncf,openvino]) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[nncf,openvino]) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[nncf,openvino]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[nncf,openvino]) (4.7.1)\n",
            "Collecting sentencepiece (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.10.1)\n",
            "Collecting accelerate (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nncf>=2.5.0 (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading nncf-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openvino-dev>=2023.0.0 (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading openvino_dev-2023.0.1-11005-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[nncf,openvino]) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->optimum[nncf,openvino])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[nncf,openvino]) (1.5.3)\n",
            "Collecting xxhash (from datasets->optimum[nncf,openvino])\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->optimum[nncf,openvino])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[nncf,openvino]) (3.8.5)\n",
            "Collecting openvino>=2023.0.0 (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading openvino-2023.0.1-11005-cp310-cp310-manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime (from optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[nncf,openvino]) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[nncf,openvino]) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[nncf,openvino]) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[nncf,openvino]) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->optimum[nncf,openvino]) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->optimum[nncf,openvino]) (0.41.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[nncf,openvino]) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece]>=4.26.0->optimum[nncf,openvino])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece]>=4.26.0->optimum[nncf,openvino])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[nncf,openvino]) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum[nncf,openvino])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[nncf,openvino]) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[nncf,openvino]) (1.3.1)\n",
            "Collecting ninja<1.11,>=1.10.0.post2 (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading ninja-1.10.2.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.3 (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting networkx<=2.8.2,>=2.6 (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing<3.0 (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymoo==0.5.0 (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading pymoo-0.5.0.tar.gz (706 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.4/706.4 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (4.3.3)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.4.2)\n",
            "Collecting jstyleson>=0.0.2 (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (8.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.2.2)\n",
            "Collecting openvino-telemetry (from nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading openvino_telemetry-2023.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.10/dist-packages (from pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.6.2)\n",
            "Collecting cma==2.7 (from pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading cma-2.7.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict>=2.4.0 (from openvino-dev>=2023.0.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino])\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev>=2023.0.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (0.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from openvino-dev>=2023.0.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (4.7.0.72)\n",
            "Requirement already satisfied: pillow>=8.1.2 in /usr/local/lib/python3.10/dist-packages (from openvino-dev>=2023.0.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[nncf,openvino]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[nncf,openvino]) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[nncf,openvino]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[nncf,openvino]) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[nncf,openvino]) (3.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (5.9.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (23.5.26)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (0.19.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[nncf,openvino]) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (3.2.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.3->pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf>=2.5.0->optimum-intel[openvino]>=1.10.1->optimum[nncf,openvino]) (1.4.4)\n",
            "Building wheels for collected packages: optimum, pymoo, optimum-intel, jstyleson\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-1.10.1-py3-none-any.whl size=358673 sha256=28d2280419e46d2018f5956f58656d09e7591fef6b677d250dd4d5e7181a17d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/2c/05/aa2df11b13d0010ec7c9ce4a9aa6ddb553e4688fb6ba3a50d3\n",
            "  Building wheel for pymoo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymoo: filename=pymoo-0.5.0-cp310-cp310-linux_x86_64.whl size=2473002 sha256=cb2b465ec22e4b8eca874f389ab6fd3a7978cd4944e7cd056ccd74065a018176\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/81/54/0ea5fd29acdfbbe43fe364b85c8a2d876a4aec07f34cf26dbc\n",
            "  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum-intel: filename=optimum_intel-1.10.1-py3-none-any.whl size=111642 sha256=7213f240c472b94054b5486fcec956d8f6c82d3de2743c2ebc8b26c89f916a85\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/64/ff/2e660924e8724382e05ce34104e1a49ad2b4492fc4f068becc\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2383 sha256=4ff3b9fa5620be919241a6975e82f09ecc99b5d49a1deecd2317b7c532936e41\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/51/c6/a1e751db88203e11c6d9ffe4683ca3d8c14b1479639bec1006\n",
            "Successfully built optimum pymoo optimum-intel jstyleson\n",
            "Installing collected packages: tokenizers, texttable, sentencepiece, safetensors, openvino-telemetry, ninja, jstyleson, cma, addict, xxhash, pyparsing, openvino, onnx, networkx, humanfriendly, dill, openvino-dev, multiprocess, huggingface-hub, coloredlogs, transformers, pymoo, onnxruntime, nncf, datasets, accelerate, optimum, optimum-intel\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.0\n",
            "    Uninstalling pyparsing-3.1.0:\n",
            "      Successfully uninstalled pyparsing-3.1.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "Successfully installed accelerate-0.21.0 addict-2.4.0 cma-2.7.0 coloredlogs-15.0.1 datasets-2.14.2 dill-0.3.7 huggingface-hub-0.16.4 humanfriendly-10.0 jstyleson-0.0.2 multiprocess-0.70.15 networkx-2.8.2 ninja-1.10.2.4 nncf-2.5.0 onnx-1.14.0 onnxruntime-1.15.1 openvino-2023.0.1 openvino-dev-2023.0.1 openvino-telemetry-2023.1.0 optimum-1.10.1 optimum-intel-1.10.1 pymoo-0.5.0 pyparsing-2.4.7 safetensors-0.3.1 sentencepiece-0.1.99 texttable-1.6.7 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.3.0\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.19.3-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.16.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.27.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.3.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.4)\n",
            "Installing collected packages: diffusers\n",
            "Successfully installed diffusers-0.19.3\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.0)\n",
            "Collecting tomesd\n",
            "  Downloading tomesd-0.1.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from tomesd) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->tomesd) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->tomesd) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->tomesd) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->tomesd) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->tomesd) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12.1->tomesd) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12.1->tomesd) (0.41.0)\n",
            "Installing collected packages: tomesd\n",
            "Successfully installed tomesd-0.1.3\n",
            "Requirement already satisfied: nncf in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: ninja<1.11,>=1.10.0.post2 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.10.2.4)\n",
            "Requirement already satisfied: texttable>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.6.7)\n",
            "Requirement already satisfied: scipy<1.11,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.10.1)\n",
            "Requirement already satisfied: networkx<=2.8.2,>=2.6 in /usr/local/lib/python3.10/dist-packages (from nncf) (2.8.2)\n",
            "Requirement already satisfied: numpy<1.24,>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.22.4)\n",
            "Requirement already satisfied: pyparsing<3.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (2.4.7)\n",
            "Requirement already satisfied: pymoo==0.5.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (0.5.0)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (4.3.3)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.4.2)\n",
            "Requirement already satisfied: jstyleson>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from nncf) (0.0.2)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.10/dist-packages (from nncf) (4.65.0)\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (8.3.1)\n",
            "Requirement already satisfied: pandas<2.1,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.2.2)\n",
            "Requirement already satisfied: openvino-telemetry in /usr/local/lib/python3.10/dist-packages (from nncf) (2023.1.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo==0.5.0->nncf) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.10/dist-packages (from pymoo==0.5.0->nncf) (1.6.2)\n",
            "Requirement already satisfied: cma==2.7 in /usr/local/lib/python3.10/dist-packages (from pymoo==0.5.0->nncf) (2.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1,>=1.1.5->nncf) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1,>=1.1.5->nncf) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf) (3.2.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.3->pymoo==0.5.0->nncf) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.1,>=1.1.5->nncf) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "264aea71a17f48db9884bf5b436f305e",
            "e80c3a737a1344ce9a56523e61b3c0fb",
            "022bf4da98d243a183afda0b8f44cb31",
            "d6540eb09cb64fcc817ba3fbb4fc629d",
            "2f6e2fbca7604abe805e00d6219edca3",
            "9a0b3d4cc8254a67a818461ff5741209",
            "d228e3921ab94590b9a0ecd088524064",
            "8dc619f4694e4123b8074c6e8716a9cb",
            "169d6f68e08d4e56b1f9ec97f5153fc6",
            "064df892b98d460b8bba045ff68f5881",
            "20bddbf9fb504fd2b3d124c2808c6b47",
            "bdc0d4e397a5434cb2f1a056de4e09db",
            "db0b44d7f70a4e4098a141365f286f2d",
            "8879af670dbb488abb29eb94dd3b8990",
            "e2df173b98424b598973fb5f72c4e583",
            "042ea99d26614ccfa25c73bacdc99108",
            "e1d58bfccb74419e8be4f13b9c9d03a1",
            "c5881b565f2f4e86bd74d1048b959061",
            "8cc9ac3e14cf4a29b742eda168904fa4",
            "69d677c871e94357bc72e1ffd25b89c0",
            "8cd32414d64942a9a2f04fb9dd90be64",
            "b59598e6b23441a7952038a4c2ae8ecd",
            "681c87b8b02e487e997e19d4a48505a9",
            "c5325e598f254ee1a64e5e5be440584f",
            "ec545cd44f1e495bb3a18d63581da1e7",
            "bec7a2fa809340b68f356303b93516b4",
            "b6e71bd7c10448139af033564ff48a8b",
            "00dd95877fd54d2a8da359195d99ed25",
            "4d534813daae44b4b83283ebabf9c21c",
            "69adfc7736f643749ab40093c4699e27",
            "79b4ccfa846240d0ab6661546d79c8d4",
            "7da841df6bfd49639a355640cf246ad9"
          ]
        },
        "id": "19x8CyqgzR_R",
        "outputId": "16fc6b6a-73c7-464b-e298-eeeaa4289dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "264aea71a17f48db9884bf5b436f305e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "OHrrMb-rzxkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"nnkhoinguyen9@gmail.com\"\n",
        "!git config --global user.name \"Zero-nnkn\""
      ],
      "metadata": {
        "id": "gL3lJdxU0h99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvrp8WV0uxeA"
      },
      "source": [
        "# OpenVINO\n",
        "- Graph optimization\n",
        "- Post-training static quantization\n",
        "- Quantization Aware Training (QAT)\n",
        "- FP16 (half precision)\n",
        "- Pruning\n",
        "- Knowledge Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7b5CXB-oGby"
      },
      "source": [
        "## ToMe + QAE + KD\n",
        "\n",
        "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/train-optimize-sd-intel/overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYCQPiu9XJ4D"
      },
      "source": [
        "### Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIkYCSZ5W8gM"
      },
      "outputs": [],
      "source": [
        "# %%writefile train_text_to_image_qat.py\n",
        "# import argparse\n",
        "# import itertools\n",
        "# import logging\n",
        "# import math\n",
        "# import os\n",
        "# import random\n",
        "# import tempfile\n",
        "# from copy import deepcopy\n",
        "# from functools import partial\n",
        "# from io import BytesIO\n",
        "# from pathlib import Path\n",
        "# from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "# import numpy as np\n",
        "# import requests\n",
        "# import tomesd\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import torch.utils.checkpoint\n",
        "# from accelerate import Accelerator\n",
        "# from accelerate.logging import get_logger\n",
        "# from accelerate.utils import set_seed\n",
        "# from datasets import load_dataset\n",
        "# from diffusers import DDIMScheduler, DDPMScheduler, DiffusionPipeline, LMSDiscreteScheduler, StableDiffusionPipeline\n",
        "# from diffusers.optimization import get_scheduler\n",
        "# from huggingface_hub import HfFolder, Repository, whoami\n",
        "# from nncf import NNCFConfig\n",
        "# from nncf.common.logging import nncf_logger\n",
        "# from nncf.torch import create_compressed_model, register_default_init_args\n",
        "# from nncf.torch.initialization import PTInitializingDataLoader\n",
        "# from nncf.torch.layer_utils import CompressionParameter\n",
        "# from openvino._offline_transformations import apply_moc_transformations, compress_quantize_weights_transformation\n",
        "# from PIL import Image\n",
        "# from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "# from torchvision import transforms\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# from optimum.exporters.onnx import export, get_stable_diffusion_models_for_export\n",
        "# from optimum.intel import OVStableDiffusionPipeline\n",
        "# from optimum.utils import (\n",
        "#     DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER,\n",
        "#     DIFFUSION_MODEL_UNET_SUBFOLDER,\n",
        "#     DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER,\n",
        "#     DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
        "# )\n",
        "\n",
        "\n",
        "# def export_models(\n",
        "#     models_and_onnx_configs: Dict[\n",
        "#         str, Tuple[Union[\"PreTrainedModel\", \"TFPreTrainedModel\", \"ModelMixin\"], \"OnnxConfig\"]\n",
        "#     ],\n",
        "#     output_dir: Path,\n",
        "#     opset: Optional[int] = None,\n",
        "#     output_names: Optional[List[str]] = None,\n",
        "#     device: str = \"cpu\",\n",
        "#     input_shapes: Optional[Dict] = None,\n",
        "#     disable_dynamic_axes_fix: Optional[bool] = False,\n",
        "#     dtype: Optional[str] = None,\n",
        "#     model_kwargs: Optional[Dict[str, Any]] = None,\n",
        "# ) -> Tuple[List[List[str]], List[List[str]]]:\n",
        "#     \"\"\"\n",
        "#     Exports a Pytorch or TensorFlow encoder decoder model to an ONNX Intermediate Representation.\n",
        "#     The following method exports the encoder and decoder components of the model as separate\n",
        "#     ONNX files.\n",
        "\n",
        "#     Args:\n",
        "#         models_and_onnx_configs (`Dict[str, Tuple[Union[`PreTrainedModel`, `TFPreTrainedModel`, `ModelMixin`], `OnnxConfig`]]):\n",
        "#             A dictionnary containing the models to export and their corresponding onnx configs.\n",
        "#         output_dir (`Path`):\n",
        "#             Output directory to store the exported ONNX models.\n",
        "#         opset (`Optional[int]`, defaults to `None`):\n",
        "#             The version of the ONNX operator set to use.\n",
        "#         output_names (`Optional[List[str]]`, defaults to `None`):\n",
        "#             The names to use for the exported ONNX files. The order must be the same as the order of submodels in the ordered dict `models_and_onnx_configs`.\n",
        "#             If None, will use the keys from `models_and_onnx_configs` as names.\n",
        "#         device (`str`, defaults to `\"cpu\"`):\n",
        "#             The device on which the ONNX model will be exported. Either `cpu` or `cuda`. Only PyTorch is supported for\n",
        "#             export on CUDA devices.\n",
        "#         input_shapes (`Optional[Dict]`, defaults to `None`):\n",
        "#             If specified, allows to use specific shapes for the example input provided to the ONNX exporter.\n",
        "#         disable_dynamic_axes_fix (`Optional[bool]`, defaults to `False`):\n",
        "#             Whether to disable the default dynamic axes fixing.\n",
        "#         dtype (`Optional[str]`, defaults to `None`):\n",
        "#             Data type to remap the model inputs to. PyTorch-only. Only `fp16` is supported.\n",
        "#         model_kwargs (`Optional[Dict[str, Any]]`, defaults to `None`):\n",
        "#             Experimental usage: keyword arguments to pass to the model during\n",
        "#             the export. This argument should be used along the `custom_onnx_config` argument\n",
        "#             in case, for example, the model inputs/outputs are changed (for example, if\n",
        "#             `model_kwargs={\"output_attentions\": True}` is passed).\n",
        "#     Returns:\n",
        "#         `Tuple[List[List[str]], List[List[str]]]`: A tuple with an ordered list of the model's inputs, and the named\n",
        "#         outputs from the ONNX configuration.\n",
        "#     \"\"\"\n",
        "#     outputs = []\n",
        "\n",
        "#     if output_names is not None and len(output_names) != len(models_and_onnx_configs):\n",
        "#         raise ValueError(\n",
        "#             f\"Provided custom names {output_names} for the export of {len(models_and_onnx_configs)} models. Please provide the same number of names as models to export.\"\n",
        "#         )\n",
        "\n",
        "#     for i, model_name in enumerate(models_and_onnx_configs.keys()):\n",
        "#         print(model_name)\n",
        "#         submodel, sub_onnx_config = models_and_onnx_configs[model_name]\n",
        "#         output_name = output_names[i] if output_names is not None else Path(model_name + \".onnx\")\n",
        "\n",
        "#         output_path = output_dir / output_name\n",
        "#         output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         outputs.append(\n",
        "#             export(\n",
        "#                 model=submodel,\n",
        "#                 config=sub_onnx_config,\n",
        "#                 output=output_path,\n",
        "#                 opset=opset,\n",
        "#                 device=device,\n",
        "#                 input_shapes=input_shapes,\n",
        "#                 disable_dynamic_axes_fix=disable_dynamic_axes_fix,\n",
        "#                 dtype=dtype,\n",
        "#                 model_kwargs=model_kwargs,\n",
        "#             )\n",
        "#         )\n",
        "\n",
        "#     outputs = list(map(list, zip(*outputs)))\n",
        "#     return outputs\n",
        "\n",
        "# requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "# random.seed(42)\n",
        "# logger = get_logger(__name__)\n",
        "# nncf_logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "# def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n",
        "#     if token is None:\n",
        "#         token = HfFolder.get_token()\n",
        "#     if organization is None:\n",
        "#         username = whoami(token)[\"name\"]\n",
        "#         return f\"{username}/{model_id}\"\n",
        "#     else:\n",
        "#         return f\"{organization}/{model_id}\"\n",
        "\n",
        "\n",
        "# def pokemon_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"image\"):\n",
        "#     image = examples[image_column]\n",
        "#     examples[\"pixel_values\"] = train_transforms(image.convert(\"RGB\"))\n",
        "#     examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "#     return examples\n",
        "\n",
        "\n",
        "# def get_pil_from_url(url):\n",
        "#     response = requests.get(url, verify=False, timeout=20)\n",
        "#     image = Image.open(BytesIO(response.content))\n",
        "#     return image.convert(\"RGB\")\n",
        "\n",
        "\n",
        "# # Many of the images in laion2B dataset are unavailable\n",
        "# # This is a workaround to substitute such images with a backup or cached available examples\n",
        "# BACKUP_PAIR = (\n",
        "#     get_pil_from_url(\n",
        "#         \"https://thumbs.dreamstime.com/t/altai-mountains-mountain-lake-russia-siberia-chuya-ridge-49130812.jpg\"\n",
        "#     ),\n",
        "#     \"Altai mountains Stock Photography\",\n",
        "# )\n",
        "# AVAILABLE_EXAMPLES = []\n",
        "\n",
        "\n",
        "# def check_text_data(data):\n",
        "#     if isinstance(data, str):\n",
        "#         return True\n",
        "#     if isinstance(data, list):\n",
        "#         return all(isinstance(x, str) for x in data)\n",
        "#     return False\n",
        "\n",
        "\n",
        "# def laion2B_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"URL\", text_column=\"TEXT\"):\n",
        "#     url = examples[image_column]\n",
        "#     try:\n",
        "#         image = get_pil_from_url(url)\n",
        "#         if not check_text_data(examples[text_column]):\n",
        "#             raise ValueError(\"Text data is not valid\")\n",
        "#         AVAILABLE_EXAMPLES.append((url, examples[text_column]))\n",
        "#     except Exception:\n",
        "#         logger.info(f\"Can't load image from url: {url}, using cache with size: {len(AVAILABLE_EXAMPLES)}\")\n",
        "#         if len(AVAILABLE_EXAMPLES) > 0:\n",
        "#             backup_id = random.randint(0, len(AVAILABLE_EXAMPLES) - 1)\n",
        "#             backup_example = AVAILABLE_EXAMPLES[backup_id]\n",
        "#             try:\n",
        "#                 image = get_pil_from_url(backup_example[0])\n",
        "#                 examples[text_column] = backup_example[1]\n",
        "#             except Exception:\n",
        "#                 logger.info(f\"Can't load image from cached url: {backup_example[0]}, using backup\")\n",
        "#                 image = BACKUP_PAIR[0].copy()\n",
        "#                 examples[text_column] = BACKUP_PAIR[1]\n",
        "#         else:\n",
        "#             logger.info(f\"Can't load image from url: {url}, using backup\")\n",
        "#             image = BACKUP_PAIR[0].copy()\n",
        "#             examples[text_column] = BACKUP_PAIR[1]\n",
        "\n",
        "#     examples[\"pixel_values\"] = train_transforms(image)\n",
        "#     examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "#     return examples\n",
        "\n",
        "\n",
        "# dataset_name_mapping = {\n",
        "#     \"lambdalabs/pokemon-blip-captions\": {\n",
        "#         \"columns\": (\"image\", \"text\"),\n",
        "#         \"preprocess_fn\": pokemon_preprocess_train,\n",
        "#         \"streaming\": False,\n",
        "#     },\n",
        "#     \"laion/laion2B-en\": {\n",
        "#         \"columns\": (\"URL\", \"TEXT\"),\n",
        "#         \"preprocess_fn\": laion2B_preprocess_train,\n",
        "#         \"streaming\": True,\n",
        "#     },\n",
        "#     \"laion/laion2B-en-aesthetic\": {\n",
        "#         \"columns\": (\"URL\", \"TEXT\"),\n",
        "#         \"preprocess_fn\": laion2B_preprocess_train,\n",
        "#         \"streaming\": True,\n",
        "#     },\n",
        "#     \"laion/laion-art\": {\n",
        "#         \"columns\": (\"URL\", \"TEXT\"),\n",
        "#         \"preprocess_fn\": laion2B_preprocess_train,\n",
        "#         \"streaming\": True,\n",
        "#     },\n",
        "#     \"laion/laion400m\": {\n",
        "#         \"columns\": (\"url\", \"caption\"),\n",
        "#         \"preprocess_fn\": partial(laion2B_preprocess_train, image_column=\"url\", text_column=\"caption\"),\n",
        "#         \"streaming\": True,\n",
        "#     },\n",
        "# }\n",
        "\n",
        "\n",
        "# # Adapted from torch-ema https://github.com/fadel/pytorch_ema/blob/master/torch_ema/ema.py#L14\n",
        "# class EMAQUnet:\n",
        "#     \"\"\"\n",
        "#     Exponential Moving Average of unets weights\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, parameters: Iterable[torch.nn.Parameter], decay=0.9999):\n",
        "#         parameters = list(parameters)\n",
        "#         self.shadow_params = [p.clone().detach() for p in parameters]\n",
        "\n",
        "#         self.decay = decay\n",
        "#         self.optimization_step = 0\n",
        "\n",
        "#     def get_decay(self, optimization_step):\n",
        "#         \"\"\"\n",
        "#         Compute the decay factor for the exponential moving average.\n",
        "#         \"\"\"\n",
        "#         value = (1 + optimization_step) / (10 + optimization_step)\n",
        "#         return 1 - min(self.decay, value)\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def step(self, parameters):\n",
        "#         parameters = list(parameters)\n",
        "\n",
        "#         self.optimization_step += 1\n",
        "#         self.decay = self.get_decay(self.optimization_step)\n",
        "\n",
        "#         for s_param, param in zip(self.shadow_params, parameters):\n",
        "#             if param.requires_grad:\n",
        "#                 tmp = param.clone()\n",
        "#                 tmp = tmp.to(s_param.device)\n",
        "#                 # tmp = self.decay * (s_param - param.clone.to(s_param.device))\n",
        "#                 tmp.sub_(s_param)\n",
        "#                 tmp.mul_(self.decay)\n",
        "#                 tmp.neg_()\n",
        "#                 s_param.sub_(tmp)\n",
        "#             else:\n",
        "#                 s_param.copy_(param)\n",
        "\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#     def copy_to(self, parameters: Iterable[torch.nn.Parameter]) -> None:\n",
        "#         \"\"\"\n",
        "#         Copy current averaged parameters into given collection of parameters.\n",
        "\n",
        "#         Args:\n",
        "#             parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
        "#                 updated with the stored moving averages. If `None`, the\n",
        "#                 parameters with which this `ExponentialMovingAverage` was\n",
        "#                 initialized will be used.\n",
        "#         \"\"\"\n",
        "#         parameters = list(parameters)\n",
        "#         for s_param, param in zip(self.shadow_params, parameters):\n",
        "#             param.data.copy_(s_param.data)\n",
        "\n",
        "#     def to(self, device=None, dtype=None) -> None:\n",
        "#         r\"\"\"Move internal buffers of the ExponentialMovingAverage to `device`.\n",
        "\n",
        "#         Args:\n",
        "#             device: like `device` argument to `torch.Tensor.to`\n",
        "#         \"\"\"\n",
        "#         # .to() on the tensors handles None correctly\n",
        "#         self.shadow_params = [\n",
        "#             p.to(device=device, dtype=dtype) if p.is_floating_point() else p.to(device=device)\n",
        "#             for p in self.shadow_params\n",
        "#         ]\n",
        "\n",
        "\n",
        "# def parse_args():\n",
        "#     parser = argparse.ArgumentParser(description=\"Stable Diffusion 8-bit Quantization for OpenVINO\")\n",
        "#     parser.add_argument(\n",
        "#         \"--model_id\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         required=True,\n",
        "#         help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--revision\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         required=False,\n",
        "#         help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--dataset_name\",\n",
        "#         type=str,\n",
        "#         default=\"lambdalabs/pokemon-blip-captions\",\n",
        "#         help=(\n",
        "#             \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n",
        "#             \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n",
        "#             \" or to a folder containing files that 🤗 Datasets can understand.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--dataset_config_name\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         help=\"The config of the Dataset, leave as None if there's only one config.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--train_data_dir\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         help=(\n",
        "#             \"A folder containing the training data. Folder contents must follow the structure described in\"\n",
        "#             \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n",
        "#             \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--max_train_samples\",\n",
        "#         type=int,\n",
        "#         default=None,\n",
        "#         help=(\n",
        "#             \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "#             \"value if set.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--output_dir\",\n",
        "#         type=str,\n",
        "#         default=\"sd-model-quantized\",\n",
        "#         help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--cache_dir\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         help=\"The directory where the downloaded models and datasets will be stored.\",\n",
        "#     )\n",
        "#     parser.add_argument(\"--seed\", type=int, default=42, help=\"A seed for reproducible training.\")\n",
        "#     parser.add_argument(\n",
        "#         \"--resolution\",\n",
        "#         type=int,\n",
        "#         default=512,\n",
        "#         help=(\n",
        "#             \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
        "#             \" resolution\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--noise_scheduler\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         choices=[\"DDIM\", \"DDPM\", \"LMSDiscrete\"],\n",
        "#         help=\"The noise scheduler for the Diffusion pipiline used for training.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--beta_start\",\n",
        "#         type=float,\n",
        "#         default=0.00085,\n",
        "#         help=\"Beta min value for noise scheduler.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--beta_end\",\n",
        "#         type=float,\n",
        "#         default=0.012,\n",
        "#         help=\"BetaMax value for noise scheduler.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--beta_schedule\",\n",
        "#         type=str,\n",
        "#         default=\"scaled_linear\",\n",
        "#         help=\"Beta schedule type\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--noise_schedule_steps\",\n",
        "#         type=int,\n",
        "#         default=1000,\n",
        "#         help=(\"The noise scheduler max train timestemps\"),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--center_crop\",\n",
        "#         default=False,\n",
        "#         action=\"store_true\",\n",
        "#         help=(\n",
        "#             \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n",
        "#             \" cropped. The images will be resized to the resolution first before cropping.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--random_flip\",\n",
        "#         action=\"store_true\",\n",
        "#         help=\"whether to randomly flip images horizontally\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--train_batch_size\", type=int, default=1, help=\"Batch size (per device) for the training dataloader.\"\n",
        "#     )\n",
        "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
        "#     parser.add_argument(\n",
        "#         \"--max_train_steps\",\n",
        "#         type=int,\n",
        "#         default=15000,\n",
        "#         help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--gradient_accumulation_steps\",\n",
        "#         type=int,\n",
        "#         default=4,\n",
        "#         help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--gradient_checkpointing\",\n",
        "#         action=\"store_true\",\n",
        "#         help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--learning_rate\",\n",
        "#         type=float,\n",
        "#         default=1e-5,\n",
        "#         help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--scale_lr\",\n",
        "#         action=\"store_true\",\n",
        "#         help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--lr_scheduler\",\n",
        "#         type=str,\n",
        "#         default=\"constant\",\n",
        "#         help=(\n",
        "#             'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
        "#             ' \"constant\", \"constant_with_warmup\"]'\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--lr_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--snr_gamma\",\n",
        "#         type=float,\n",
        "#         default=None,\n",
        "#         help=\"SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. \"\n",
        "#         \"More details here: https://arxiv.org/abs/2303.09556.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--allow_tf32\",\n",
        "#         action=\"store_true\",\n",
        "#         help=(\n",
        "#             \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
        "#             \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--ema_device\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         choices=[\"cpu\", \"cuda\"],\n",
        "#         help=\"Whether to use EMA model and where to store the EMA model.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--quantization_mode\",\n",
        "#         type=str,\n",
        "#         default=\"moderate\",\n",
        "#         choices=[\"moderate\", \"aggressive\"],\n",
        "#         help=(\n",
        "#             \"'aggressive' mode quantizes all MatMul operations while 'moderate' keeps MatMul that applies attention mask non-quantized.\"\n",
        "#             \" The later allows preserving a better accuracy while keeping the similar inference performance after optimization.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--non_ema_revision\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         required=False,\n",
        "#         help=(\n",
        "#             \"Revision of pretrained non-ema model identifier. Must be a branch, tag or git identifier of the local or\"\n",
        "#             \" remote repository specified with --pretrained_model_name_or_path.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--dataloader_num_workers\",\n",
        "#         type=int,\n",
        "#         default=0,\n",
        "#         help=(\n",
        "#             \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n",
        "#     parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n",
        "#     parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
        "#     parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n",
        "#     parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "#     parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n",
        "#     parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n",
        "#     parser.add_argument(\n",
        "#         \"--hub_model_id\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--logging_dir\",\n",
        "#         type=str,\n",
        "#         default=\"logs\",\n",
        "#         help=(\n",
        "#             \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
        "#             \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--mixed_precision\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         choices=[\"no\", \"fp16\", \"bf16\"],\n",
        "#         help=(\n",
        "#             \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
        "#             \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
        "#             \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--report_to\",\n",
        "#         type=str,\n",
        "#         default=\"tensorboard\",\n",
        "#         help=(\n",
        "#             'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
        "#             ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "#     parser.add_argument(\n",
        "#         \"--checkpointing_steps\",\n",
        "#         type=int,\n",
        "#         default=15000,\n",
        "#         help=(\n",
        "#             \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\"\n",
        "#             \" training using `--resume_from_checkpoint`.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--checkpoints_total_limit\",\n",
        "#         type=int,\n",
        "#         default=1,\n",
        "#         help=(\n",
        "#             \"Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`.\"\n",
        "#             \" See Accelerator::save_state https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state\"\n",
        "#             \" for more docs\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--resume_from_checkpoint\",\n",
        "#         type=str,\n",
        "#         default=None,\n",
        "#         help=(\n",
        "#             \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
        "#             ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--tome_ratio\",\n",
        "#         type=float,\n",
        "#         default=0,\n",
        "#         help=(\n",
        "#             \"Token Merging ratio. If 0, no merging is applied\" \"More details here: https://arxiv.org/abs/2303.17604.\"\n",
        "#         ),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--opt_init_steps\",\n",
        "#         type=int,\n",
        "#         default=300,\n",
        "#         help=(\"Max number of initialization steps for quantization before the actual fine-tuning.\"),\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--opt_init_type\",\n",
        "#         type=str,\n",
        "#         default=\"mean_min_max\",\n",
        "#         choices=[\"min_max\", \"mean_min_max\", \"threesigma\"],\n",
        "#         help=\"They way how to estimate activation quantization paramters at the initializatin step before QAT.\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--tune_quantizers_only\",\n",
        "#         action=\"store_true\",\n",
        "#         default=False,\n",
        "#         help=\"Whether to train quantization parameters only.\",\n",
        "#     )\n",
        "#     parser.add_argument(\"--use_kd\", action=\"store_true\", help=\"Use Knowledge Distillation to boost accuracy.\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "#     env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
        "#     if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
        "#         args.local_rank = env_local_rank\n",
        "\n",
        "#     # Sanity checks\n",
        "#     if args.dataset_name is None and args.train_data_dir is None:\n",
        "#         raise ValueError(\"Need either a dataset name or a training folder.\")\n",
        "\n",
        "#     # default to using the same revision for the non-ema model if not specified\n",
        "#     if args.non_ema_revision is None:\n",
        "#         args.non_ema_revision = args.revision\n",
        "#     return args\n",
        "\n",
        "\n",
        "# def get_noise_scheduler(args):\n",
        "#     scheduler_args = {\n",
        "#         \"beta_start\": args.beta_start,\n",
        "#         \"beta_end\": args.beta_end,\n",
        "#         \"beta_schedule\": args.beta_schedule,\n",
        "#         \"num_train_timesteps\": args.noise_schedule_steps,\n",
        "#     }\n",
        "#     if args.noise_scheduler == \"DDIM\":\n",
        "#         noise_scheduler = DDIMScheduler(**scheduler_args)\n",
        "#     elif args.noise_scheduler == \"DDPM\":\n",
        "#         noise_scheduler = DDPMScheduler(**scheduler_args)\n",
        "#     elif args.noise_scheduler == \"LMSDiscrete\":\n",
        "#         noise_scheduler = LMSDiscreteScheduler(**scheduler_args)\n",
        "#     else:\n",
        "#         raise ValueError(f\"Unknown noise schedule {args.noise_schedule}\")\n",
        "#     return noise_scheduler\n",
        "\n",
        "\n",
        "# def export_to_onnx(pipeline, save_dir, device='cpu', opset=14):\n",
        "#     unet = pipeline.unet\n",
        "#     vae = pipeline.vae\n",
        "#     text_encoder = pipeline.text_encoder\n",
        "#     unet.eval().to(device)\n",
        "#     vae.eval().to(device)\n",
        "#     text_encoder.eval().to(device)\n",
        "\n",
        "#     ONNX_WEIGHTS_NAME = \"model.onnx\"\n",
        "\n",
        "#     output_names = [\n",
        "#         os.path.join(DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "#         os.path.join(DIFFUSION_MODEL_UNET_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "#         os.path.join(DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "#         os.path.join(DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "#     ]\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         models_and_onnx_configs = get_stable_diffusion_models_for_export(pipeline)\n",
        "#         pipeline.save_config(save_dir)\n",
        "#         export_models(\n",
        "#             models_and_onnx_configs=models_and_onnx_configs, output_dir=Path(save_dir), output_names=output_names, device=device, opset=14\n",
        "#         )\n",
        "\n",
        "\n",
        "# def export_to_openvino(pipeline, onnx_dir, save_dir):\n",
        "#     ov_pipe = OVStableDiffusionPipeline.from_pretrained(\n",
        "#         model_id=onnx_dir,\n",
        "#         from_onnx=True,\n",
        "#         model_save_dir=save_dir,\n",
        "#         tokenizer=pipeline.tokenizer,\n",
        "#         scheduler=pipeline.scheduler,\n",
        "#         feature_extractor=pipeline.feature_extractor,\n",
        "#         compile=False,\n",
        "#     )\n",
        "#     apply_moc_transformations(ov_pipe.unet.model, cf=False)\n",
        "#     compress_quantize_weights_transformation(ov_pipe.unet.model)\n",
        "#     ov_pipe.save_pretrained(save_dir)\n",
        "\n",
        "\n",
        "# class UnetInitDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, data):\n",
        "#         super().__init__()\n",
        "#         self.init_data = data\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.init_data)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.init_data[index]\n",
        "\n",
        "\n",
        "# def prepare_nncf_init_data(pipeline, dataloader, args):\n",
        "#     weight_dtype = torch.float32\n",
        "#     text_encoder = pipeline.text_encoder\n",
        "#     vae = pipeline.vae\n",
        "#     noise_scheduler = pipeline.scheduler\n",
        "\n",
        "#     nncf_init_data = []\n",
        "\n",
        "#     logger.info(f\"Fetching {args.opt_init_steps} for the initialization...\")\n",
        "#     for _, batch in tqdm(zip(range(args.opt_init_steps), itertools.islice(dataloader, 0, args.opt_init_steps))):\n",
        "#         with torch.no_grad():\n",
        "#             # Convert images to latent space\n",
        "#             latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
        "#             latents = latents * 0.18215\n",
        "\n",
        "#             # Sample noise that we'll add to the latents\n",
        "#             noise = torch.randn_like(latents)\n",
        "#             bsz = latents.shape[0]\n",
        "#             # Sample a random timestep for each image\n",
        "#             timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
        "#             timesteps = timesteps.long()\n",
        "\n",
        "#             # Add noise to the latents according to the noise magnitude at each timestep\n",
        "#             # (this is the forward diffusion process)\n",
        "#             noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "#             encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "#             nncf_init_data.append(\n",
        "#                 (\n",
        "#                     torch.squeeze(noisy_latents).to(\"cpu\"),\n",
        "#                     torch.squeeze(timesteps).to(\"cpu\"),\n",
        "#                     torch.squeeze(encoder_hidden_states).to(\"cpu\"),\n",
        "#                     0,\n",
        "#                 )\n",
        "#             )\n",
        "#     return nncf_init_data\n",
        "\n",
        "\n",
        "# # The config should work for Stable Diffusion v1.4-2.1\n",
        "# def get_nncf_config(pipeline, dataloader, args):\n",
        "#     text_encoder = pipeline.text_encoder\n",
        "#     unet = pipeline.unet\n",
        "#     moderate_quantization_config = {\n",
        "#         \"input_info\": [\n",
        "#             {  # \"keyword\": \"latent_model_input\",\n",
        "#                 \"sample_size\": [1, unet.config[\"in_channels\"], unet.config[\"sample_size\"], unet.config[\"sample_size\"]]\n",
        "#             },\n",
        "#             {\"sample_size\": [1]},  # \"keyword\": \"t\",\n",
        "#             {  # \"keyword\": \"encoder_hidden_states\",\n",
        "#                 \"sample_size\": [1, text_encoder.config.max_position_embeddings, text_encoder.config.hidden_size]\n",
        "#             },\n",
        "#         ],\n",
        "#         \"log_dir\": args.output_dir,  # The log directory for NNCF-specific logging outputs.\n",
        "#         \"compression\": [\n",
        "#             {\n",
        "#                 \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
        "#                 \"preset\": \"mixed\",\n",
        "#                 \"initializer\": {\n",
        "#                     \"range\": {\"num_init_samples\": args.opt_init_steps, \"type\": args.opt_init_type},\n",
        "#                     \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": args.opt_init_steps},\n",
        "#                 },\n",
        "#                 \"scope_overrides\": {\"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}}},\n",
        "#                 \"ignored_scopes\": [\n",
        "#                     \"{re}.*__add___[0-2]\",\n",
        "#                     \"{re}.*layer_norm_0\",\n",
        "#                     \"{re}.*Attention.*/bmm_0\",\n",
        "#                     \"{re}.*__truediv__*\",\n",
        "#                     \"{re}.*group_norm_0\",\n",
        "#                     \"{re}.*mul___[0-2]\",\n",
        "#                     \"{re}.*silu_[0-2]\",\n",
        "#                 ],\n",
        "#                 \"overflow_fix\": \"disable\",\n",
        "#                 \"export_to_onnx_standard_ops\": True,\n",
        "#             },\n",
        "#         ],\n",
        "#     }\n",
        "\n",
        "#     aggressive_quantization_config = {\n",
        "#         \"input_info\": [\n",
        "#             {  # \"keyword\": \"latent_model_input\",\n",
        "#                 \"sample_size\": [1, unet.config[\"in_channels\"], unet.config[\"sample_size\"], unet.config[\"sample_size\"]]\n",
        "#             },\n",
        "#             {\"sample_size\": [1]},  # \"keyword\": \"t\",\n",
        "#             {  # \"keyword\": \"encoder_hidden_states\",\n",
        "#                 \"sample_size\": [1, text_encoder.config.max_position_embeddings, text_encoder.config.hidden_size]\n",
        "#             },\n",
        "#         ],\n",
        "#         \"log_dir\": args.output_dir,  # The log directory for NNCF-specific logging outputs.\n",
        "#         \"compression\": [\n",
        "#             {\n",
        "#                 \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
        "#                 \"preset\": \"mixed\",\n",
        "#                 \"initializer\": {\n",
        "#                     \"range\": {\"num_init_samples\": args.opt_init_steps, \"type\": args.opt_init_type},\n",
        "#                     \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": args.opt_init_steps},\n",
        "#                 },\n",
        "#                 \"scope_overrides\": {\n",
        "#                     \"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}, \"{re}.*bmm_0\": {\"mode\": \"symmetric\"}}\n",
        "#                 },\n",
        "#                 \"ignored_scopes\": [\n",
        "#                     \"{re}.*layer_norm_0\",\n",
        "#                     \"{re}.*__truediv__*\",\n",
        "#                     \"{re}.*group_norm_0\",\n",
        "#                     \"{re}.*mul___[0-2]\",\n",
        "#                     \"{re}.*silu_[0-2]\",\n",
        "#                 ],\n",
        "#                 \"overflow_fix\": \"disable\",\n",
        "#                 \"export_to_onnx_standard_ops\": True,\n",
        "#             },\n",
        "#         ],\n",
        "#     }\n",
        "\n",
        "#     class UnetInitDataLoader(PTInitializingDataLoader):\n",
        "#         def get_inputs(self, dataloader_output):\n",
        "#             noisy_latents = dataloader_output[0].float().to(unet.device, non_blocking=True)\n",
        "#             timesteps = dataloader_output[1].float().to(unet.device, non_blocking=True)\n",
        "#             encoder_hidden_states = dataloader_output[2].float().to(unet.device, non_blocking=True)\n",
        "#             return (noisy_latents, timesteps, encoder_hidden_states), {}\n",
        "\n",
        "#         def get_target(self, dataloader_output):\n",
        "#             return dataloader_output[0]\n",
        "\n",
        "#     quantization_config = (\n",
        "#         aggressive_quantization_config if args.quantization_mode == \"aggressive\" else moderate_quantization_config\n",
        "#     )\n",
        "\n",
        "#     nncf_config = NNCFConfig.from_dict(quantization_config)\n",
        "#     nncf_config = register_default_init_args(nncf_config, UnetInitDataLoader(dataloader))\n",
        "#     # nncf_config = register_default_init_args(nncf_config, dataloader)\n",
        "#     return nncf_config\n",
        "\n",
        "\n",
        "# def collate_fn(examples, preprocess_fn, tokenizer):\n",
        "#     examples = [preprocess_fn(example) for example in examples]\n",
        "#     pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "#     pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "#     input_ids = [example[\"input_ids\"] for example in examples]\n",
        "#     padded_tokens = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n",
        "#     return {\n",
        "#         \"pixel_values\": pixel_values,\n",
        "#         \"input_ids\": padded_tokens.input_ids,\n",
        "#         \"attention_mask\": padded_tokens.attention_mask,\n",
        "#     }\n",
        "\n",
        "# # We need to tokenize input captions and transform the images.\n",
        "# def tokenize_captions(examples, tokenizer, caption_column, is_train=True):\n",
        "#     captions = []\n",
        "#     caption = examples[caption_column]\n",
        "#     if isinstance(caption, str):\n",
        "#         captions.append(caption)\n",
        "#     elif isinstance(caption, (list, np.ndarray)):\n",
        "#         # take a random caption if there are multiple\n",
        "#         captions.append(random.choice(caption) if is_train else caption[0])\n",
        "#     else:\n",
        "#         raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
        "#     inputs = tokenizer(captions[0], max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
        "#     input_ids = inputs.input_ids\n",
        "#     return input_ids\n",
        "\n",
        "# def main():\n",
        "#     args = parse_args()\n",
        "#     logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
        "\n",
        "#     weight_dtype = torch.float32\n",
        "#     if args.mixed_precision == \"fp16\":\n",
        "#         weight_dtype = torch.float16\n",
        "#     elif args.mixed_precision == \"bf16\":\n",
        "#         weight_dtype = torch.bfloat16\n",
        "\n",
        "#     accelerator = Accelerator(\n",
        "#         gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "#         mixed_precision=args.mixed_precision,\n",
        "#         log_with=args.report_to,\n",
        "#         project_dir=logging_dir,\n",
        "#     )\n",
        "\n",
        "#     logging.basicConfig(\n",
        "#         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "#         datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "#         level=logging.INFO,\n",
        "#     )\n",
        "\n",
        "#     logger.info(accelerator.state, main_process_only=False)\n",
        "\n",
        "#     # If passed along, set the training seed now.\n",
        "#     if args.seed is not None:\n",
        "#         set_seed(args.seed)\n",
        "\n",
        "#     # Handle the repository creation\n",
        "#     if accelerator.is_main_process:\n",
        "#         if args.push_to_hub:\n",
        "#             if args.hub_model_id is None:\n",
        "#                 repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
        "#             else:\n",
        "#                 repo_name = args.hub_model_id\n",
        "#             repo = Repository(args.output_dir, clone_from=repo_name)\n",
        "\n",
        "#             with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "#                 if \"step_*\" not in gitignore:\n",
        "#                     gitignore.write(\"step_*\\n\")\n",
        "#                 if \"epoch_*\" not in gitignore:\n",
        "#                     gitignore.write(\"epoch_*\\n\")\n",
        "#         elif args.output_dir is not None:\n",
        "#             os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "#     pipeline = DiffusionPipeline.from_pretrained(args.model_id)\n",
        "\n",
        "#     if args.use_kd:\n",
        "#         teacher_model = deepcopy(pipeline.unet)\n",
        "\n",
        "#     if args.tome_ratio > 0:\n",
        "#         logger.info(f\"Using Token Merging with ratio: {args.tome_ratio}\")\n",
        "#         tomesd.apply_patch(\n",
        "#             pipeline, ratio=args.tome_ratio, use_rand=False\n",
        "#         )  # Can also use pipe.unet in place of pipe here\n",
        "\n",
        "#     # Load models and create wrapper for stable diffusion\n",
        "#     tokenizer = pipeline.tokenizer\n",
        "#     text_encoder = pipeline.text_encoder\n",
        "#     vae = pipeline.vae\n",
        "#     unet = pipeline.unet\n",
        "#     noise_scheduler = pipeline.scheduler\n",
        "#     if args.noise_scheduler:\n",
        "#         noise_scheduler = get_noise_scheduler(args)\n",
        "\n",
        "#     # Freeze vae and text_encoder\n",
        "#     vae.requires_grad_(False)\n",
        "#     text_encoder.requires_grad_(False)\n",
        "\n",
        "#     if args.gradient_checkpointing:\n",
        "#         unet.enable_gradient_checkpointing()\n",
        "\n",
        "#     if args.scale_lr:\n",
        "#         args.learning_rate = (\n",
        "#             args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
        "#         )\n",
        "\n",
        "#     # Initialize the optimizer\n",
        "#     if args.use_8bit_adam:\n",
        "#         try:\n",
        "#             import bitsandbytes as bnb\n",
        "#         except ImportError:\n",
        "#             raise ImportError(\n",
        "#                 \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
        "#             )\n",
        "\n",
        "#         optimizer_cls = bnb.optim.AdamW8bit\n",
        "#     else:\n",
        "#         optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "#     # Get the datasets: you can either provide your own training and evaluation files (see below)\n",
        "#     # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
        "\n",
        "#     # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
        "#     # download the dataset.\n",
        "#     dataset_settings = dataset_name_mapping.get(args.dataset_name, None)\n",
        "#     if dataset_settings is None:\n",
        "#         raise ValueError(\n",
        "#             f\"Dataset {args.dataset_name} not supported. Please choose from {dataset_name_mapping.keys()}\"\n",
        "#         )\n",
        "\n",
        "#     if args.dataset_name is not None:\n",
        "#         # Downloading and loading a dataset from the hub.\n",
        "#         dataset = load_dataset(\n",
        "#             args.dataset_name,\n",
        "#             args.dataset_config_name,\n",
        "#             cache_dir=args.cache_dir,\n",
        "#             streaming=dataset_settings[\"streaming\"],\n",
        "#         )\n",
        "#     else:\n",
        "#         data_files = {}\n",
        "#         if args.train_data_dir is not None:\n",
        "#             data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n",
        "#         dataset = load_dataset(\n",
        "#             \"imagefolder\",\n",
        "#             data_files=data_files,\n",
        "#             cache_dir=args.cache_dir,\n",
        "#         )\n",
        "#         # See more about loading custom images at\n",
        "#         # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n",
        "\n",
        "#     # Preprocessing the datasets.\n",
        "#     # We need to tokenize inputs and targets.\n",
        "\n",
        "#     # 6. Get the column names for input/target.\n",
        "#     dataset_columns = dataset_settings[\"columns\"]\n",
        "#     caption_column = dataset_columns[1]\n",
        "\n",
        "#     # Preprocessing the datasets.\n",
        "#     train_transforms = transforms.Compose(\n",
        "#         [\n",
        "#             transforms.Resize((args.resolution, args.resolution), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "#             transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
        "#             transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize([0.5], [0.5]),\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "#     preprocess_fn = partial(\n",
        "#         dataset_settings[\"preprocess_fn\"], train_transforms=train_transforms, tokenize_captions=partial(tokenize_captions, tokenizer=tokenizer, caption_column=caption_column)\n",
        "#     )\n",
        "\n",
        "#     with accelerator.main_process_first():\n",
        "#         if args.max_train_samples is not None:\n",
        "#             dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42, buffer_size=args.max_train_samples)\n",
        "#         # Set the training transforms\n",
        "#         train_dataset = dataset[\"train\"]\n",
        "\n",
        "#     train_dataloader = torch.utils.data.DataLoader(\n",
        "#         train_dataset, collate_fn=partial(collate_fn, preprocess_fn=preprocess_fn, tokenizer=tokenizer), batch_size=args.train_batch_size, num_workers=args.dataloader_num_workers\n",
        "#     )\n",
        "\n",
        "#     unet = accelerator.prepare(unet)\n",
        "#     vae.to(unet.device)\n",
        "#     text_encoder.to(unet.device)\n",
        "#     train_dataloader = accelerator.prepare_data_loader(train_dataloader)\n",
        "#     orig_unet = unet  # save link to original unet model for EMA\n",
        "\n",
        "#     ## Create initialization dataset for PTQ\n",
        "#     nncf_init_data = prepare_nncf_init_data(pipeline, train_dataloader, args)\n",
        "#     init_dataloader = torch.utils.data.DataLoader(UnetInitDataset(nncf_init_data), batch_size=1, num_workers=1)\n",
        "#     nncf_config = get_nncf_config(pipeline, init_dataloader, args)\n",
        "\n",
        "#     # Quantize the model and initialize quantizer using init data\n",
        "#     compression_controller, unet = create_compressed_model(unet, nncf_config)\n",
        "\n",
        "#     statistics_unet = compression_controller.statistics()\n",
        "#     logger.info(statistics_unet.to_str())\n",
        "\n",
        "#     del nncf_init_data, init_dataloader\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     unet.train()\n",
        "\n",
        "#     if args.tune_quantizers_only:\n",
        "#         for p in unet.parameters():\n",
        "#             if not isinstance(p, CompressionParameter):\n",
        "#                 p.requires_grad = False\n",
        "\n",
        "#     # Reinit\n",
        "#     optimizer = optimizer_cls(\n",
        "#         filter(lambda p: p.requires_grad, unet.parameters()),\n",
        "#         lr=args.learning_rate,\n",
        "#         betas=(args.adam_beta1, args.adam_beta2),\n",
        "#         weight_decay=args.adam_weight_decay,\n",
        "#         eps=args.adam_epsilon,\n",
        "#     )\n",
        "#     lr_scheduler = get_scheduler(\n",
        "#         args.lr_scheduler,\n",
        "#         optimizer=optimizer,\n",
        "#         num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "#         num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        "#     )\n",
        "\n",
        "#     # Scheduler and math around the number of training steps.\n",
        "#     overrode_max_train_steps = False\n",
        "#     dataset_len = args.max_train_samples if args.max_train_samples is not None else len(train_dataloader)\n",
        "#     num_update_steps_per_epoch = math.ceil(dataset_len / args.gradient_accumulation_steps)\n",
        "#     if args.max_train_steps is None:\n",
        "#         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "#         overrode_max_train_steps = True\n",
        "\n",
        "#     unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "#         unet, optimizer, train_dataloader, lr_scheduler\n",
        "#     )\n",
        "\n",
        "#     # Move text_encode and vae to gpu.\n",
        "#     # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
        "#     # as these models are only used for inference, keeping weights in full precision is not required.\n",
        "#     text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "#     vae.to(accelerator.device, dtype=weight_dtype)\n",
        "#     if args.use_kd:\n",
        "#       teacher_model.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "#     # Create EMA for the unet.\n",
        "#     if args.ema_device:\n",
        "#         ema_unet = EMAQUnet(orig_unet.parameters())\n",
        "#         if args.ema_device == \"cpu\":\n",
        "#             ema_unet.to(\"cpu\")\n",
        "\n",
        "#     # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "#     num_update_steps_per_epoch = math.ceil(dataset_len / args.gradient_accumulation_steps)\n",
        "#     if overrode_max_train_steps:\n",
        "#         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "#     # Afterwards we recalculate our number of training epochs\n",
        "#     args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "#     # We need to initialize the trackers we use, and also store our configuration.\n",
        "#     # The trackers initializes automatically on the main process.\n",
        "#     if accelerator.is_main_process:\n",
        "#         accelerator.init_trackers(\"text2image-fine-tune\", config=vars(args))\n",
        "\n",
        "#     # Train!\n",
        "#     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "#     logger.info(\"***** Running training *****\")\n",
        "#     logger.info(f\"  Num examples = {dataset_len}\")\n",
        "#     logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "#     logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "#     logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "#     logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "#     logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "\n",
        "#     # Only show the progress bar once on each machine.\n",
        "#     progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "#     progress_bar.set_description(\"Steps\")\n",
        "#     global_step = 0\n",
        "\n",
        "#     for epoch in range(args.num_train_epochs):\n",
        "#         train_loss = 0.0\n",
        "#         compression_controller.scheduler.epoch_step()\n",
        "\n",
        "#         for step, batch in enumerate(train_dataloader):\n",
        "#             with accelerator.accumulate(unet):\n",
        "#                 # Convert images to latent space\n",
        "#                 latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
        "#                 latents = latents * 0.18215\n",
        "\n",
        "#                 # Sample noise that we'll add to the latents\n",
        "#                 noise = torch.randn_like(latents)\n",
        "#                 bsz = latents.shape[0]\n",
        "#                 # Sample a random timestep for each image\n",
        "#                 timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
        "#                 timesteps = timesteps.long()\n",
        "\n",
        "#                 # Add noise to the latents according to the noise magnitude at each timestep\n",
        "#                 # (this is the forward diffusion process)\n",
        "#                 noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "#                 # Get the text embedding for conditioning\n",
        "#                 encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "#                 # Predict the noise residual and compute loss\n",
        "#                 noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "#                 loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
        "\n",
        "#                 if args.use_kd:\n",
        "#                     with torch.no_grad():\n",
        "#                         orig_output = teacher_model(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "#                     loss += F.mse_loss(noise_pred.float(), orig_output.float(), reduction=\"mean\")\n",
        "\n",
        "#                 # Gather the losses across all processes for logging (if we use distributed training).\n",
        "#                 avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
        "#                 train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
        "\n",
        "#                 compression_loss_unet = compression_controller.loss()\n",
        "#                 loss = loss + compression_loss_unet\n",
        "\n",
        "#                 # Backpropagate\n",
        "#                 accelerator.backward(loss)\n",
        "#                 if accelerator.sync_gradients:\n",
        "#                     accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "#                 optimizer.step()\n",
        "#                 lr_scheduler.step()\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#             # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "#             if accelerator.sync_gradients:\n",
        "#                 if args.ema_device:\n",
        "#                     ema_unet.step(orig_unet.parameters())\n",
        "#                 progress_bar.update(1)\n",
        "#                 global_step += 1\n",
        "#                 accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
        "#                 train_loss = 0.0\n",
        "\n",
        "#             logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "#             progress_bar.set_postfix(**logs)\n",
        "\n",
        "#             if global_step >= args.max_train_steps:\n",
        "#                 break\n",
        "\n",
        "#     if args.use_kd:\n",
        "#         del teacher_model\n",
        "#     # Create the pipeline using the trained modules and save it.\n",
        "#     accelerator.wait_for_everyone()\n",
        "#     if accelerator.is_main_process:\n",
        "#         unet = accelerator.unwrap_model(unet)\n",
        "#         if args.ema_device:\n",
        "#             ema_unet.copy_to(orig_unet.parameters())\n",
        "\n",
        "#     accelerator.end_training()\n",
        "\n",
        "#     # Export optimized pipline to OpenVINO\n",
        "#     export_unet = compression_controller.strip(do_copy=False)\n",
        "#     export_pipeline = StableDiffusionPipeline(\n",
        "#         text_encoder=text_encoder,\n",
        "#         vae=vae,\n",
        "#         unet=export_unet,\n",
        "#         tokenizer=tokenizer,\n",
        "#         scheduler=noise_scheduler,\n",
        "#         safety_checker=pipeline.safety_checker,\n",
        "#         feature_extractor=pipeline.feature_extractor,\n",
        "#     )\n",
        "\n",
        "#     print('Save model')\n",
        "#     export_pipeline.save_pretrained(Path(args.output_dir))\n",
        "\n",
        "#     if args.push_to_hub:\n",
        "#         repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n",
        "#     # print('Export to ONNX')\n",
        "#     # export_to_onnx(export_pipeline, Path(args.output_dir) / \"onnx\", device='cuda', opset=14)\n",
        "#     # print('Export to OpenVINO')\n",
        "#     # export_to_openvino(export_pipeline, Path(args.output_dir) / \"onnx\", Path(args.output_dir) / \"openvino\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YlVbDi4XHxm"
      },
      "source": [
        "### Run\n",
        "- svjack/Stable-Diffusion-Pokemon-en\n",
        "- justinpinkney/pokemon-stable-diffusion\n",
        "- --gradient_checkpointing \\ not support\n",
        "- --use_kd \\\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZmb8ai1neG8"
      },
      "outputs": [],
      "source": [
        "# !python train_text_to_image_qat.py \\\n",
        "#     --ema_device=\"cpu\" \\\n",
        "#     --model_id=\"svjack/Stable-Diffusion-Pokemon-en\" \\\n",
        "#     --center_crop \\\n",
        "#     --random_flip \\\n",
        "#     --train_batch_size=1 \\\n",
        "#     --dataloader_num_workers=1 \\\n",
        "#     --dataset_name=\"lambdalabs/pokemon-blip-captions\" \\\n",
        "#     --max_train_steps=0 \\\n",
        "#     --opt_init_steps=300 \\\n",
        "#     --tome_ratio=0.5 \\\n",
        "#     --quantization_mode=\"moderate\" \\\n",
        "#     --mixed_precision=\"no\" \\\n",
        "#     --output_dir=sd-pokemon-static-quantized-tome \\\n",
        "#     --hub_model_id Zero-nnkn/sd-pokemon-static-quantized-tome \\\n",
        "#     --push_to_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQ + ToMe"
      ],
      "metadata": {
        "id": "BzpAAkYR4LNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tomesd\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from optimum.intel import OVConfig, OVQuantizer\n",
        "\n",
        "SEED = 42\n",
        "TOME_RATIO = 0.5"
      ],
      "metadata": {
        "id": "dgDrZXqc9GV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268d2fb2-9769-4a15-bf61-d7fa1b431e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"svjack/Stable-Diffusion-Pokemon-en\"\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "482dbace0783495ab2b2609e8a15b060",
            "e84d05fdfd6947c8891fb9e72c0544ea",
            "33b8ad6c1b0240e08a9fe37b2392a832",
            "64ee9ba97fe241a0ada1a31d48fc79d6",
            "be8e5438b2324bd19fa757d2098e0659",
            "43a8f4d09fda42f3b22906b94cfff34b",
            "2ce4a4c6067843818215ec1e7df5cbd4",
            "95ab42408e8c4c569d3e7d04c46a19bf",
            "0189322624b8443998c05cb29c212da5",
            "24feca8d6efd4beb80148c390298c385",
            "63a1c7e608054b7f8f85a13937dac3b9"
          ]
        },
        "id": "HTBs0qjpB5U6",
        "outputId": "5aa255f5-b250-4343-b945-21ce655b280a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "safety_checker/model.safetensors not found\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "482dbace0783495ab2b2609e8a15b060"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:128: FutureWarning: The configuration file of this scheduler: LMSDiscreteScheduler {\n",
            "  \"_class_name\": \"LMSDiscreteScheduler\",\n",
            "  \"_diffusers_version\": \"0.19.3\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"steps_offset\": 0,\n",
            "  \"timestep_spacing\": \"linspace\",\n",
            "  \"trained_betas\": null,\n",
            "  \"use_karras_sigmas\": false\n",
            "}\n",
            " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tomesd.apply_patch(\n",
        "    pipeline, ratio=TOME_RATIO, use_rand=False,\n",
        ")  # Can also use pipe.unet in place of pipe here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SGQOAgaX8jx",
        "outputId": "df19c54a-797e-49fd-b020-1362a42cfb8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StableDiffusionPipeline {\n",
              "  \"_class_name\": \"StableDiffusionPipeline\",\n",
              "  \"_diffusers_version\": \"0.19.3\",\n",
              "  \"_name_or_path\": \"svjack/Stable-Diffusion-Pokemon-en\",\n",
              "  \"feature_extractor\": [\n",
              "    \"transformers\",\n",
              "    \"CLIPImageProcessor\"\n",
              "  ],\n",
              "  \"requires_safety_checker\": true,\n",
              "  \"safety_checker\": [\n",
              "    \"stable_diffusion\",\n",
              "    \"StableDiffusionSafetyChecker\"\n",
              "  ],\n",
              "  \"scheduler\": [\n",
              "    \"diffusers\",\n",
              "    \"LMSDiscreteScheduler\"\n",
              "  ],\n",
              "  \"text_encoder\": [\n",
              "    \"transformers\",\n",
              "    \"CLIPTextModel\"\n",
              "  ],\n",
              "  \"tokenizer\": [\n",
              "    \"transformers\",\n",
              "    \"CLIPTokenizer\"\n",
              "  ],\n",
              "  \"unet\": [\n",
              "    \"diffusers\",\n",
              "    \"UNet2DConditionModel\"\n",
              "  ],\n",
              "  \"vae\": [\n",
              "    \"diffusers\",\n",
              "    \"AutoencoderKL\"\n",
              "  ]\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantizer\n",
        "tokenizer = pipeline.tokenizer\n",
        "unet = pipeline.unet\n",
        "# quantization_config = OVConfig()\n",
        "# quantizer = OVQuantizer.from_pretrained(pipeline.unet)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from torchvision import transforms\n",
        "from transformers import DataCollator\n",
        "\n",
        "NUM_SAMPLES = 100\n",
        "BATCH_SIZE = 1\n",
        "INIT_TYPE = 'min_max' # [\"min_max\", \"mean_min_max\", \"threesigma\"]"
      ],
      "metadata": {
        "id": "6SoenC0EMXss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def tokenize_captions(examples, tokenizer, is_train=True):\n",
        "    captions = []\n",
        "    caption = examples[caption_column]\n",
        "    if isinstance(caption, str):\n",
        "        captions.append(caption)\n",
        "    elif isinstance(caption, (list, np.ndarray)):\n",
        "        # take a random caption if there are multiple\n",
        "        captions.append(random.choice(caption) if is_train else caption[0])\n",
        "    else:\n",
        "        raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
        "    inputs = tokenizer(captions[0], max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
        "    input_ids = inputs.input_ids\n",
        "    return input_ids\n",
        "\n",
        "def preprocess_dataset(examples, tokenizer, transforms, image_column=\"image\"):\n",
        "    image = examples[image_column]\n",
        "    examples[\"pixel_values\"] = transforms(image.convert(\"RGB\"))\n",
        "    examples[\"input_ids\"] = tokenize_captions(examples, tokenizer=tokenizer)\n",
        "    return examples\n",
        "\n",
        "def collate_fn(examples, preprocess_fn, tokenizer):\n",
        "    examples = [preprocess_fn(example) for example in examples]\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "    input_ids = [example[\"input_ids\"] for example in examples]\n",
        "    padded_tokens = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\")\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": padded_tokens.input_ids,\n",
        "        \"attention_mask\": padded_tokens.attention_mask,\n",
        "    }\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.CenterCrop(512),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset_settings = {\n",
        "  'columns': ('image', 'text'),\n",
        "}\n",
        "dataset_columns = dataset_settings['columns']\n",
        "caption_column = dataset_columns[1]\n",
        "preprocess_fn = partial(preprocess_dataset, tokenizer=tokenizer, transforms=train_transforms)\n",
        "\n",
        "# Get dataset\n",
        "dataset = load_dataset(\n",
        "        'lambdalabs/pokemon-blip-captions',\n",
        "        split='train',\n",
        "        cache_dir=None,\n",
        "    )\n",
        "dataset = dataset.shuffle(seed=SEED).select(range(NUM_SAMPLES))\n",
        "\n",
        "# Get dataloader\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(SEED)\n",
        "sampler = RandomSampler(dataset, generator=generator)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, collate_fn=partial(collate_fn, preprocess_fn=preprocess_fn, tokenizer=tokenizer))"
      ],
      "metadata": {
        "id": "FaK24x3TZM-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unet"
      ],
      "metadata": {
        "id": "n6DEVSvXaiUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nncf import NNCFConfig\n",
        "from nncf.torch import create_compressed_model, register_default_init_args\n",
        "from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk\n",
        "from nncf.torch.initialization import PTInitializingDataLoader\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "metadata": {
        "id": "PaxA8Cfn8eUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetDataset(Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        super().__init__()\n",
        "        self.data = data_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "\n",
        "class UnetInitDataLoader(PTInitializingDataLoader):\n",
        "    def __init__(self, data_loader, device: str):\n",
        "        super().__init__(data_loader)\n",
        "        self.device = device\n",
        "\n",
        "    def get_inputs(self, dataloader_output):\n",
        "        noisy_latents = dataloader_output[1].float().to(self.device, non_blocking=True)\n",
        "        timesteps = dataloader_output[2].float().to(self.device, non_blocking=True)\n",
        "        encoder_hidden_states = dataloader_output[3].float().to(self.device, non_blocking=True)\n",
        "        return (noisy_latents, timesteps, encoder_hidden_states), {}\n",
        "\n",
        "    def get_target(self, dataloader_output):\n",
        "        return dataloader_output[0]\n",
        "\n",
        "\n",
        "def prepare_unet_data(pipeline, dataloader, device='cpu'):\n",
        "    weight_dtype = torch.float32\n",
        "    text_encoder = pipeline.text_encoder.to(device)\n",
        "    vae = pipeline.vae.to(device)\n",
        "    noise_scheduler = pipeline.scheduler\n",
        "\n",
        "    unet_data = []\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        with torch.no_grad():\n",
        "            # Convert images to latent space\n",
        "            latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype).to(device)).latent_dist.sample()\n",
        "            latents = latents * 0.18215\n",
        "\n",
        "            # Sample noise that we'll add to the latents\n",
        "            noise = torch.randn_like(latents)\n",
        "            bsz = latents.shape[0]\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
        "            timesteps = timesteps.long()\n",
        "\n",
        "            # Add noise to the latents according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "            encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(device))[0]\n",
        "\n",
        "            for i in range(len(noisy_latents)):\n",
        "                unet_data.append(\n",
        "                    (\n",
        "                        noise[i].to('cpu'),\n",
        "                        noisy_latents[i].to('cpu'),\n",
        "                        timesteps[i].to('cpu'),\n",
        "                        encoder_hidden_states[i].to('cpu'),\n",
        "                    )\n",
        "                )\n",
        "    return unet_data\n",
        "\n",
        "unet_dataset = UnetDataset(prepare_unet_data(pipeline, dataloader, device=DEVICE))\n",
        "unet_dataloader = DataLoader(unet_dataset, batch_size=BATCH_SIZE)\n",
        "unet_init_dataloader = UnetInitDataLoader(unet_dataloader, device=DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KT__NUksADT",
        "outputId": "5edaf965-3672-4b70-86f6-3ff11f491cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'LMSDiscreteScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'LMSDiscreteScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "100%|██████████| 100/100 [00:32<00:00,  3.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_COMPRESSION = {\n",
        "    \"algorithm\": \"quantization\",\n",
        "    \"preset\": \"mixed\",\n",
        "    \"initializer\": {\n",
        "        \"range\": {\"num_init_samples\": NUM_SAMPLES, \"type\": INIT_TYPE},\n",
        "        \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": 0},\n",
        "    },\n",
        "    \"scope_overrides\": {\"activations\": {\"{re}.*matmul_0\": {\"mode\": \"symmetric\"}}},\n",
        "    \"ignored_scopes\": [\n",
        "        \"{re}.*Embedding.*\",\n",
        "        \"{re}.*add___.*\",\n",
        "        \"{re}.*layer_norm_.*\",\n",
        "        \"{re}.*matmul_1\",\n",
        "        \"{re}.*__truediv__.*\",\n",
        "    ],\n",
        "    \"overflow_fix\": \"disable\",\n",
        "    \"export_to_onnx_standard_ops\": True,\n",
        "}\n",
        "\n",
        "INT8_WEIGHT_COMPRESSION = {\n",
        "    \"algorithm\": \"quantization\",\n",
        "    \"weights\": {\n",
        "        \"mode\": \"symmetric\",\n",
        "        \"bits\": 8,\n",
        "        \"target_scopes\": [\n",
        "            \"{re}.*Embedding.*\",\n",
        "            \"{re}.*matmul_.*\",\n",
        "            \"{re}.*addmm_.*\",\n",
        "            \"{re}.*baddmm_.*\",\n",
        "            \"{re}.*linear_.*\",\n",
        "        ],\n",
        "        \"ignored_scopes\": [\n",
        "            \"{re}.*conv_*\",\n",
        "        ],\n",
        "    },\n",
        "    \"activations\": {\n",
        "        \"ignored_scopes\": [\n",
        "            \"{re}.*add___.*\",\n",
        "            \"{re}.*__radd___.*\",\n",
        "            \"{re}.*layer_norm_.*\",\n",
        "            \"{re}.*__truediv__.*\",\n",
        "            \"{re}.*__mul___.*\",\n",
        "            \"{re}.*__rmul___.*\",\n",
        "            \"{re}.*tanh_.*\",\n",
        "            \"{re}.*pow_.*\",\n",
        "            \"{re}.*matmul_.*\",\n",
        "            \"{re}.*addmm_.*\",\n",
        "            \"{re}.*baddmm_.*\",\n",
        "            \"{re}.*linear_.*\",\n",
        "            \"{re}.*conv_.*\",\n",
        "        ],\n",
        "    },\n",
        "    \"overflow_fix\": \"disable\",\n",
        "    \"export_to_onnx_standard_ops\": True,\n",
        "}\n",
        "\n",
        "MODERATE_COMPRESSION =  {\n",
        "    \"algorithm\": \"quantization\",\n",
        "    \"preset\": \"mixed\",\n",
        "    \"initializer\": {\n",
        "        \"range\": {\"num_init_samples\": NUM_SAMPLES, \"type\": INIT_TYPE},\n",
        "        \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\":  NUM_SAMPLES},\n",
        "    },\n",
        "    \"scope_overrides\": {\"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}}},\n",
        "    \"ignored_scopes\": [\n",
        "        \"{re}.*__add___[0-2]\",\n",
        "        \"{re}.*layer_norm_0\",\n",
        "        \"{re}.*Attention.*/bmm_0\",\n",
        "        \"{re}.*__truediv__*\",\n",
        "        \"{re}.*group_norm_0\",\n",
        "        \"{re}.*mul___[0-2]\",\n",
        "        \"{re}.*silu_[0-2]\",\n",
        "    ],\n",
        "    \"overflow_fix\": \"disable\",\n",
        "    \"export_to_onnx_standard_ops\": True,\n",
        "}\n",
        "\n",
        "AGGRESSIVE_COMPRESSION = {\n",
        "    \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
        "    \"preset\": \"mixed\",\n",
        "    \"initializer\": {\n",
        "        \"range\": {\"num_init_samples\": NUM_SAMPLES, \"type\": INIT_TYPE},\n",
        "        \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\":  NUM_SAMPLES},\n",
        "    },\n",
        "    \"scope_overrides\": {\n",
        "        \"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}, \"{re}.*bmm_0\": {\"mode\": \"symmetric\"}}\n",
        "    },\n",
        "    \"ignored_scopes\": [\n",
        "        \"{re}.*layer_norm_0\",\n",
        "        \"{re}.*__truediv__*\",\n",
        "        \"{re}.*group_norm_0\",\n",
        "        \"{re}.*mul___[0-2]\",\n",
        "        \"{re}.*silu_[0-2]\",\n",
        "    ],\n",
        "    \"overflow_fix\": \"disable\",\n",
        "    \"export_to_onnx_standard_ops\": True,\n",
        "}\n",
        "\n",
        "\n",
        "def prepare_unet_quantize_config(pipeline, compressions):\n",
        "    config = {\n",
        "        \"input_info\": [\n",
        "            {  # \"keyword\": \"latent_model_input\",\n",
        "                \"sample_size\": [1, pipeline.unet.config[\"in_channels\"], pipeline.unet.config[\"sample_size\"], pipeline.unet.config[\"sample_size\"]]\n",
        "            },\n",
        "            {\"sample_size\": [1]},  # \"keyword\": \"t\",\n",
        "            {  # \"keyword\": \"encoder_hidden_states\",\n",
        "                \"sample_size\": [1, pipeline.text_encoder.config.max_position_embeddings, pipeline.text_encoder.config.hidden_size]\n",
        "            },\n",
        "        ],\n",
        "        \"compression\": [c for c in compressions],\n",
        "    }\n",
        "\n",
        "    return config\n",
        "\n",
        "quantization_config = prepare_unet_quantize_config(pipeline, compressions=[MODERATE_COMPRESSION])"
      ],
      "metadata": {
        "id": "EBhthT5ekTCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(model, quantization_config, init_dataloader):\n",
        "    nncf_config = NNCFConfig.from_dict(quantization_config)\n",
        "    nncf_config = register_default_init_args(nncf_config, init_dataloader)\n",
        "    controller, compressed_model = create_compressed_model(\n",
        "        model, nncf_config, wrap_inputs_fn=wrap_nncf_model_inputs_with_objwalk\n",
        "    )\n",
        "    compressed_model = controller.strip(do_copy=False)\n",
        "    return compressed_model"
      ],
      "metadata": {
        "id": "b7XwvX665mmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-q36QzHCn-r",
        "outputId": "e25719fe-e336-4254-9ad6-e2d3d1afe338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet2DConditionModel(\n",
              "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (time_proj): Timesteps()\n",
              "  (time_embedding): TimestepEmbedding(\n",
              "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
              "    (act): SiLU()\n",
              "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "  )\n",
              "  (down_blocks): ModuleList(\n",
              "    (0): CrossAttnDownBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): CrossAttnDownBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): CrossAttnDownBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): DownBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up_blocks): ModuleList(\n",
              "    (0): UpBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): CrossAttnUpBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): CrossAttnUpBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): CrossAttnUpBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Transformer2DModel(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): ToMeBlock(\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
              "                (to_out): ModuleList(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff): FeedForward(\n",
              "                (net): ModuleList(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mid_block): UNetMidBlock2DCrossAttn(\n",
              "    (attentions): ModuleList(\n",
              "      (0): Transformer2DModel(\n",
              "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (transformer_blocks): ModuleList(\n",
              "          (0): ToMeBlock(\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn1): Attention(\n",
              "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn2): Attention(\n",
              "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
              "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
              "              (to_out): ModuleList(\n",
              "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff): FeedForward(\n",
              "              (net): ModuleList(\n",
              "                (0): GEGLU(\n",
              "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (resnets): ModuleList(\n",
              "      (0): ResnetBlock2D(\n",
              "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (nonlinearity): SiLU()\n",
              "      )\n",
              "      (1): ResnetBlock2D(\n",
              "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (nonlinearity): SiLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "  (conv_act): SiLU()\n",
              "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_unet = quantize(unet, quantization_config=quantization_config, init_dataloader=unet_init_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqU17kUM63Rj",
        "outputId": "025f1c47-c848-4c16-c2a5-6f4882393ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nncf/torch/dynamic_graph/wrappers.py:175: UserWarning: scatter_reduce() is in beta and the API may change at any time. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1615.)\n",
            "  result = operator(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:Not adding activation input quantizer for operation: 20 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "21 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 8 UNet2DConditionModel/Timesteps[time_proj]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 9 UNet2DConditionModel/Timesteps[time_proj]/__rmul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1043 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1123 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1205 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 131 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1315 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1425 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 23 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 240 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 319 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 398 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 477 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 556 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 569 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 582 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 660 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 674 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 689 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 704 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 721 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 801 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 881 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 963 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 26 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 27 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "28 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 31 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 32 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 33 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 50 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 91 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 38 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 42 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__matmul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 107 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 113 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 114 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 115 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 119 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 122 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 127 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 128 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "129 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 134 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 135 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "136 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 139 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 140 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 141 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 158 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 199 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 146 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 150 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__matmul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 215 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 221 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 222 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 223 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 227 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 230 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 235 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 237 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "238 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 243 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 244 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "245 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 249 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 250 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 251 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 255 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 271 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 277 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 278 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 279 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 295 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 301 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 302 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 303 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 307 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 310 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 315 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 316 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "317 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 322 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 323 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "324 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 327 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 328 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 329 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 333 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 349 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 355 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 356 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 357 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 373 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 379 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 380 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 381 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 385 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 388 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 393 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 395 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "396 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 401 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 402 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "403 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 407 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 408 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 409 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 413 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 429 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 435 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 436 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 437 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 453 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 459 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 460 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 461 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 465 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 468 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 473 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 474 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "475 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 480 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 481 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "482 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 485 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 486 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 487 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 491 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 507 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 513 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 514 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 515 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 531 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 537 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 538 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 539 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 543 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 546 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 551 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 553 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "554 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 559 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 560 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "561 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 564 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 565 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 566 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "567 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 572 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 573 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "574 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 577 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 578 UNet2DConditionModel/ModuleList[down_blocks]/DownBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 579 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "580 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 585 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 586 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "587 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 590 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 591 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 592 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 596 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 612 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 618 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 619 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 620 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 636 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 642 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 643 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 644 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 648 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 651 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 656 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 657 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "658 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 663 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 664 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "665 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 668 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 669 UNet2DConditionModel/UNetMidBlock2DCrossAttn[mid_block]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 671 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "672 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 677 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 678 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "679 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 683 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 684 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 686 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "687 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 692 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 693 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "694 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 698 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 699 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 701 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "702 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 707 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 708 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "709 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 713 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 714 UNet2DConditionModel/ModuleList[up_blocks]/UpBlock2D[0]/ModuleList[resnets]/ResnetBlock2D[2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 718 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "719 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 724 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 725 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "726 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 730 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 731 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 732 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 736 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 752 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 758 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 759 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 760 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 776 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 782 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 783 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 784 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 788 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 791 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 796 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 798 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "799 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 804 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 805 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "806 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 810 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 811 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 812 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 816 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 832 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 838 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 839 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 840 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 856 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 862 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 863 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 864 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 868 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 871 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 876 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 878 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "879 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 884 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 885 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "886 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 890 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 891 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[resnets]/ResnetBlock2D[2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 892 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 896 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 912 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 918 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 919 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 920 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 936 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 942 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 943 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 944 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 948 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 951 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 956 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[1]/ModuleList[attentions]/Transformer2DModel[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 960 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "961 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 966 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 967 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "968 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 972 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 973 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 974 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 978 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 994 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1000 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1001 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1002 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1018 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1024 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1025 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1026 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1030 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1033 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1038 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1040 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "1041 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1046 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1047 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "1048 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1052 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1053 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1054 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1058 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1074 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1080 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1081 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1082 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1098 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1104 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1105 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1106 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1110 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1113 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1118 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1120 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "1121 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1126 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1127 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "1128 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1132 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1133 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[resnets]/ResnetBlock2D[2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1134 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1138 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1154 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1160 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1161 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1162 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1178 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1184 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1185 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1186 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1190 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1193 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___2\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1198 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[2]/ModuleList[attentions]/Transformer2DModel[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1202 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "1203 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1208 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1209 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "1210 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1214 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1215 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1216 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1233 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1274 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1221 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1225 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__matmul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1290 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1296 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1297 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1298 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1302 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1305 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1310 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1312 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "1313 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1318 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1319 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "1320 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1324 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1325 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1326 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1343 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1384 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1331 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1335 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__matmul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1400 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1406 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1407 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1408 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1412 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1415 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1420 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1422 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm1]/group_norm_0\n",
            "1423 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1428 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1429 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/NNCFGroupNorm[norm2]/group_norm_0\n",
            "1430 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/SiLU[nonlinearity]/silu_2\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1434 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1435 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[resnets]/ResnetBlock2D[2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1436 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/NNCFGroupNorm[norm]/group_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1453 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm1]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1494 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm2]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1441 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1445 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__matmul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1510 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1516 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn2]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1517 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1518 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/NNCFLayerNorm[norm3]/layer_norm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1522 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/FeedForward[ff]/ModuleList[net]/GEGLU[0]/__mul___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1525 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/__add___1\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1530 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/__add___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1531 UNet2DConditionModel/NNCFGroupNorm[conv_norm_out]/group_norm_0\n",
            "1532 UNet2DConditionModel/SiLU[conv_act]/silu_0\n",
            "\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1258 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1264 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1368 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1374 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1478 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 1484 UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 183 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 189 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 75 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/bmm_0\n",
            "INFO:nncf:Not adding activation input quantizer for operation: 81 UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/Attention[attn1]/__truediv___0\n",
            "INFO:nncf:Collecting tensor statistics |█               | 11 / 100\n",
            "INFO:nncf:Collecting tensor statistics |███             | 22 / 100\n",
            "INFO:nncf:Collecting tensor statistics |█████           | 33 / 100\n",
            "INFO:nncf:Collecting tensor statistics |███████         | 44 / 100\n",
            "INFO:nncf:Collecting tensor statistics |████████        | 55 / 100\n",
            "INFO:nncf:Collecting tensor statistics |██████████      | 66 / 100\n",
            "INFO:nncf:Collecting tensor statistics |████████████    | 77 / 100\n",
            "INFO:nncf:Collecting tensor statistics |██████████████  | 88 / 100\n",
            "INFO:nncf:Collecting tensor statistics |███████████████ | 99 / 100\n",
            "INFO:nncf:Collecting tensor statistics |████████████████| 100 / 100\n",
            "INFO:nncf:Scales will be unified for quantizer group:\n",
            "UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/gather_5|INPUT0\n",
            "UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/scatter_reduce_0|OUTPUT\n",
            "\n",
            "INFO:nncf:Scales will be unified for quantizer group:\n",
            "UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/gather_5|INPUT0\n",
            "UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/scatter_reduce_0|OUTPUT\n",
            "\n",
            "INFO:nncf:Scales will be unified for quantizer group:\n",
            "UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/gather_5|INPUT0\n",
            "UNet2DConditionModel/ModuleList[up_blocks]/CrossAttnUpBlock2D[3]/ModuleList[attentions]/Transformer2DModel[2]/ModuleList[transformer_blocks]/ToMeBlock[0]/scatter_reduce_0|OUTPUT\n",
            "\n",
            "INFO:nncf:Scales will be unified for quantizer group:\n",
            "UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/gather_5|INPUT0\n",
            "UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[1]/ModuleList[transformer_blocks]/ToMeBlock[0]/scatter_reduce_0|OUTPUT\n",
            "\n",
            "INFO:nncf:Scales will be unified for quantizer group:\n",
            "UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/gather_5|INPUT0\n",
            "UNet2DConditionModel/ModuleList[down_blocks]/CrossAttnDownBlock2D[0]/ModuleList[attentions]/Transformer2DModel[0]/ModuleList[transformer_blocks]/ToMeBlock[0]/scatter_reduce_0|OUTPUT\n",
            "\n",
            "INFO:nncf:Scales will be unified for quantizer group:\n",
            "UNet2DConditionModel/Timesteps[time_proj]/cos_0|OUTPUT\n",
            "UNet2DConditionModel/Timesteps[time_proj]/sin_0|OUTPUT\n",
            "\n",
            "INFO:nncf:Compiling and loading torch extension: quantized_functions_cuda...\n",
            "INFO:nncf:Finished loading torch extension: quantized_functions_cuda\n",
            "INFO:nncf:BatchNorm statistics adaptation |█               | 11 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |███             | 22 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |█████           | 33 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |███████         | 44 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████        | 55 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |██████████      | 66 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████████    | 77 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |██████████████  | 88 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |███████████████ | 99 / 100\n",
            "INFO:nncf:BatchNorm statistics adaptation |████████████████| 100 / 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nncf/torch/nncf_network.py:938: FutureWarning: Old style of accessing NNCF-specific attributes and methods on NNCFNetwork objects is deprecated. Access the NNCF-specific attrs through the NNCFInterface, which is set up as an `nncf` attribute on the compressed model object.\n",
            "For instance, instead of `compressed_model.get_graph()` you should now write `compressed_model.nncf.get_graph()`.\n",
            "The old style will be removed after NNCF v2.5.0\n",
            "  warning_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Export"
      ],
      "metadata": {
        "id": "CX2St8Z3UmZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "exported_pipeline = StableDiffusionPipeline(\n",
        "    text_encoder=pipeline.text_encoder,\n",
        "    vae=pipeline.vae,\n",
        "    unet=compressed_unet,\n",
        "    tokenizer=pipeline.tokenizer,\n",
        "    scheduler=pipeline.scheduler,\n",
        "    safety_checker=pipeline.safety_checker,\n",
        "    feature_extractor=pipeline.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "MzI1brofUosM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### SDPipeline"
      ],
      "metadata": {
        "id": "Tukqp-6BT4WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exported_pipeline.save_pretrained(save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWGDhSGwGzoo",
        "outputId": "d234282d-89b8-4efb-c443-91b1e47d75ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/sd-pokemon-quantized-tome  is already a clone of https://huggingface.co/Zero-nnkn/sd-pokemon-quantized-tome. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/sd-pokemon-quantized-tome  is already a clone of https://huggingface.co/Zero-nnkn/sd-pokemon-quantized-tome. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ONNX"
      ],
      "metadata": {
        "id": "q3pdXxs4T-YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from optimum.exporters.onnx import export_models, get_stable_diffusion_models_for_export\n",
        "from optimum.intel import OVStableDiffusionPipeline\n",
        "from optimum.utils import (\n",
        "    DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER,\n",
        "    DIFFUSION_MODEL_UNET_SUBFOLDER,\n",
        "    DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER,\n",
        "    DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER,\n",
        ")\n",
        "\n",
        "def export_to_onnx(pipeline, save_dir):\n",
        "    save_dir = Path(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    unet = pipeline.unet\n",
        "    vae = pipeline.vae\n",
        "    text_encoder = pipeline.text_encoder\n",
        "\n",
        "    unet.eval()\n",
        "    vae.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    ONNX_WEIGHTS_NAME = \"model.onnx\"\n",
        "\n",
        "    output_names = [\n",
        "        os.path.join(DIFFUSION_MODEL_TEXT_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "        os.path.join(DIFFUSION_MODEL_UNET_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "        os.path.join(DIFFUSION_MODEL_VAE_ENCODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "        os.path.join(DIFFUSION_MODEL_VAE_DECODER_SUBFOLDER, ONNX_WEIGHTS_NAME),\n",
        "    ]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        models_and_onnx_configs = get_stable_diffusion_models_for_export(pipeline)\n",
        "        pipeline.save_config(save_dir)\n",
        "        export_models(\n",
        "            models_and_onnx_configs=models_and_onnx_configs, output_dir=Path(save_dir), output_names=output_names, device='cuda'\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jm4MYHPMRfzr",
        "outputId": "b34d1e3b-f42d-40b5-abd3-fb835c7356ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/sd-pokemon-static-quantized-tome-onnx is already a clone of https://huggingface.co/Zero-nnkn/sd-pokemon-static-quantized-tome-onnx. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/sd-pokemon-static-quantized-tome-onnx is already a clone of https://huggingface.co/Zero-nnkn/sd-pokemon-static-quantized-tome-onnx. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "Using framework PyTorch: 1.13.1+cu117\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:286: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:294: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:326: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5408: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:65: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "Using framework PyTorch: 1.13.1+cu117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:nncf:You are setting `forward` on an NNCF-processed model object.\n",
            "NNCF relies on custom-wrapping the `forward` call in order to function properly.\n",
            "Arbitrary adjustments to the forward function on an NNCFNetwork object have undefined behaviour.\n",
            "If you need to replace the underlying forward function of the original model so that NNCF should be using that instead of the original forward function that NNCF saved during the compressed model creation, you can do this by calling:\n",
            "model.nncf.set_original_unbound_forward(fn)\n",
            "if `fn` has an unbound 0-th `self` argument, or\n",
            "with model.nncf.temporary_bound_original_forward(fn): ...\n",
            "if `fn` already had 0-th `self` argument bound or never had it in the first place.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_condition.py:752: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
            "/usr/local/lib/python3.10/dist-packages/tomesd/patch.py:13: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  downsample = int(math.ceil(math.sqrt(original_tokens // x.shape[1])))\n",
            "/usr/local/lib/python3.10/dist-packages/tomesd/patch.py:18: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  w = int(math.ceil(original_w / downsample))\n",
            "/usr/local/lib/python3.10/dist-packages/tomesd/patch.py:19: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  h = int(math.ceil(original_h / downsample))\n",
            "/usr/local/lib/python3.10/dist-packages/tomesd/patch.py:20: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  r = int(x.shape[1] * args[\"ratio\"])\n",
            "/usr/local/lib/python3.10/dist-packages/tomesd/patch.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  use_rand = False if x.shape[0] % 2 == 1 else args[\"use_rand\"]\n",
            "/usr/local/lib/python3.10/dist-packages/tomesd/merge.py:89: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  r = min(a.shape[1], r)\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py:215: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert hidden_states.shape[1] == self.channels\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py:220: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert hidden_states.shape[1] == self.channels\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py:139: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert hidden_states.shape[1] == self.channels\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py:152: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if hidden_states.shape[0] >= 64:\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_condition.py:991: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if not return_dict:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:nncf:You are setting `forward` on an NNCF-processed model object.\n",
            "NNCF relies on custom-wrapping the `forward` call in order to function properly.\n",
            "Arbitrary adjustments to the forward function on an NNCFNetwork object have undefined behaviour.\n",
            "If you need to replace the underlying forward function of the original model so that NNCF should be using that instead of the original forward function that NNCF saved during the compressed model creation, you can do this by calling:\n",
            "model.nncf.set_original_unbound_forward(fn)\n",
            "if `fn` has an unbound 0-th `self` argument, or\n",
            "with model.nncf.temporary_bound_original_forward(fn): ...\n",
            "if `fn` already had 0-th `self` argument bound or never had it in the first place.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnsupportedOperatorError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-39304e3f5e9c>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sd-pokemon-static-quantized-tome-onnx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Zero-nnkn/sd-pokemon-static-quantized-tome-onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mexport_to_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexported_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sd-pokemon-static-quantized-tome-onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-39304e3f5e9c>\u001b[0m in \u001b[0;36mexport_to_onnx\u001b[0;34m(pipeline, save_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodels_and_onnx_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stable_diffusion_models_for_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         export_models(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mmodels_and_onnx_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_and_onnx_configs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/exporters/onnx/convert.py\u001b[0m in \u001b[0;36mexport_models\u001b[0;34m(models_and_onnx_configs, output_dir, opset, output_names, device, input_shapes, disable_dynamic_axes_fix, dtype, model_kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         outputs.append(\n\u001b[0;32m--> 760\u001b[0;31m             export(\n\u001b[0m\u001b[1;32m    761\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub_onnx_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/exporters/onnx/convert.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, config, output, opset, device, input_shapes, disable_dynamic_axes_fix, dtype, model_kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported dtype, supported dtypes are: `torch.float16`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         export_output = export_pytorch(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/exporters/onnx/convert.py\u001b[0m in \u001b[0;36mexport_pytorch\u001b[0;34m(model, config, opset, output, device, dtype, input_shapes, model_kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;31m# Export can work with named args but the dict containing named args has to be the last element of the args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;31m# tuple.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                 onnx_export(\n\u001b[0m\u001b[1;32m    581\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \"\"\"\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         graph = _optimize_graph(\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputsSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         raise errors.UnsupportedOperatorError(\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m             \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::scatter_reduce' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "R7b5CXB-oGby"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "264aea71a17f48db9884bf5b436f305e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cd32414d64942a9a2f04fb9dd90be64",
              "IPY_MODEL_b59598e6b23441a7952038a4c2ae8ecd",
              "IPY_MODEL_681c87b8b02e487e997e19d4a48505a9",
              "IPY_MODEL_c5325e598f254ee1a64e5e5be440584f"
            ],
            "layout": "IPY_MODEL_d228e3921ab94590b9a0ecd088524064"
          }
        },
        "e80c3a737a1344ce9a56523e61b3c0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dc619f4694e4123b8074c6e8716a9cb",
            "placeholder": "​",
            "style": "IPY_MODEL_169d6f68e08d4e56b1f9ec97f5153fc6",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "022bf4da98d243a183afda0b8f44cb31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_064df892b98d460b8bba045ff68f5881",
            "placeholder": "​",
            "style": "IPY_MODEL_20bddbf9fb504fd2b3d124c2808c6b47",
            "value": ""
          }
        },
        "d6540eb09cb64fcc817ba3fbb4fc629d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_bdc0d4e397a5434cb2f1a056de4e09db",
            "style": "IPY_MODEL_db0b44d7f70a4e4098a141365f286f2d",
            "value": true
          }
        },
        "2f6e2fbca7604abe805e00d6219edca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8879af670dbb488abb29eb94dd3b8990",
            "style": "IPY_MODEL_e2df173b98424b598973fb5f72c4e583",
            "tooltip": ""
          }
        },
        "9a0b3d4cc8254a67a818461ff5741209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_042ea99d26614ccfa25c73bacdc99108",
            "placeholder": "​",
            "style": "IPY_MODEL_e1d58bfccb74419e8be4f13b9c9d03a1",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d228e3921ab94590b9a0ecd088524064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8dc619f4694e4123b8074c6e8716a9cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "169d6f68e08d4e56b1f9ec97f5153fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "064df892b98d460b8bba045ff68f5881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20bddbf9fb504fd2b3d124c2808c6b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdc0d4e397a5434cb2f1a056de4e09db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db0b44d7f70a4e4098a141365f286f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8879af670dbb488abb29eb94dd3b8990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2df173b98424b598973fb5f72c4e583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "042ea99d26614ccfa25c73bacdc99108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d58bfccb74419e8be4f13b9c9d03a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5881b565f2f4e86bd74d1048b959061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cc9ac3e14cf4a29b742eda168904fa4",
            "placeholder": "​",
            "style": "IPY_MODEL_69d677c871e94357bc72e1ffd25b89c0",
            "value": "Connecting..."
          }
        },
        "8cc9ac3e14cf4a29b742eda168904fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d677c871e94357bc72e1ffd25b89c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cd32414d64942a9a2f04fb9dd90be64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec545cd44f1e495bb3a18d63581da1e7",
            "placeholder": "​",
            "style": "IPY_MODEL_bec7a2fa809340b68f356303b93516b4",
            "value": "Token is valid (permission: write)."
          }
        },
        "b59598e6b23441a7952038a4c2ae8ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e71bd7c10448139af033564ff48a8b",
            "placeholder": "​",
            "style": "IPY_MODEL_00dd95877fd54d2a8da359195d99ed25",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "681c87b8b02e487e997e19d4a48505a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d534813daae44b4b83283ebabf9c21c",
            "placeholder": "​",
            "style": "IPY_MODEL_69adfc7736f643749ab40093c4699e27",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "c5325e598f254ee1a64e5e5be440584f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b4ccfa846240d0ab6661546d79c8d4",
            "placeholder": "​",
            "style": "IPY_MODEL_7da841df6bfd49639a355640cf246ad9",
            "value": "Login successful"
          }
        },
        "ec545cd44f1e495bb3a18d63581da1e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec7a2fa809340b68f356303b93516b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6e71bd7c10448139af033564ff48a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00dd95877fd54d2a8da359195d99ed25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d534813daae44b4b83283ebabf9c21c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69adfc7736f643749ab40093c4699e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79b4ccfa846240d0ab6661546d79c8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7da841df6bfd49639a355640cf246ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "482dbace0783495ab2b2609e8a15b060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e84d05fdfd6947c8891fb9e72c0544ea",
              "IPY_MODEL_33b8ad6c1b0240e08a9fe37b2392a832",
              "IPY_MODEL_64ee9ba97fe241a0ada1a31d48fc79d6"
            ],
            "layout": "IPY_MODEL_be8e5438b2324bd19fa757d2098e0659"
          }
        },
        "e84d05fdfd6947c8891fb9e72c0544ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a8f4d09fda42f3b22906b94cfff34b",
            "placeholder": "​",
            "style": "IPY_MODEL_2ce4a4c6067843818215ec1e7df5cbd4",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "33b8ad6c1b0240e08a9fe37b2392a832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ab42408e8c4c569d3e7d04c46a19bf",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0189322624b8443998c05cb29c212da5",
            "value": 7
          }
        },
        "64ee9ba97fe241a0ada1a31d48fc79d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24feca8d6efd4beb80148c390298c385",
            "placeholder": "​",
            "style": "IPY_MODEL_63a1c7e608054b7f8f85a13937dac3b9",
            "value": " 7/7 [00:32&lt;00:00,  5.42s/it]"
          }
        },
        "be8e5438b2324bd19fa757d2098e0659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a8f4d09fda42f3b22906b94cfff34b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ce4a4c6067843818215ec1e7df5cbd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95ab42408e8c4c569d3e7d04c46a19bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0189322624b8443998c05cb29c212da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24feca8d6efd4beb80148c390298c385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a1c7e608054b7f8f85a13937dac3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}